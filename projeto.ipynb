{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Visualização de Dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurações de Visualização\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d8db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um CSV em memória para simular um arquivo local\n",
    "csv_data = \"\"\"id_imovel,data_anuncio,bairro,area_m2,quartos,vagas_garagem,preco\n",
    "1,2023-01-15,Centro,120,3,2,650000\n",
    "2,2023-01-20,Zona Leste,85,2,1,450000\n",
    "3,2023-02-10,centro,150,4,3,800000\n",
    "4,2023-02-12,Zona Sul,70,,1,380000\n",
    "5,2023-03-05,Zona Oeste,200,4,4,1200000\n",
    "6,03/20/2023,Zona Leste,95,3,1,480000\n",
    "7,2023-04-01,Zona Sul,110,3,2,580000\n",
    "8,2023-04-30,Centro,300,5,5,15000000\n",
    "9,2023-05-15,Zona Oeste,60,1,1,320000\n",
    "10,2023-05-20,Zona Leste,NaN,2,1,400000\n",
    "11,2023-06-11,Centro,130,3,2,680000\n",
    "12,2023-06-25,Zona Sul,90,2,2,NaN\n",
    "\"\"\"\n",
    "\n",
    "# Lendo o CSV para um DataFrame pandas\n",
    "df_imoveis = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "print(\"### Dados de Imóveis (CSV Local) ###\")\n",
    "print(df_imoveis.info())\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "print(df_imoveis.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6551c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selic_rate(start_date='01/01/2023', end_date='31/12/2023'):\n",
    "    \"\"\"\n",
    "    Busca a série temporal da taxa SELIC mensal na API do Banco Central do Brasil.\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Data de início no formato 'dd/mm/yyyy'.\n",
    "        end_date (str): Data de fim no formato 'dd/mm/yyyy'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com as colunas 'data' e 'taxa_selic'.\n",
    "    \"\"\"\n",
    "    url = f'https://api.bcb.gov.br/dados/serie/bcdata.sgs.432/dados?formato=json&dataInicial={start_date}&dataFinal={end_date}'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Lança exceção para códigos de erro HTTP\n",
    "        data = response.json()\n",
    "        df = pd.DataFrame(data)\n",
    "        df.rename(columns={'data': 'data_selic', 'valor': 'taxa_selic'}, inplace=True)\n",
    "        df['taxa_selic'] = df['taxa_selic'].astype(float)\n",
    "        df['data_selic'] = pd.to_datetime(df['data_selic'], dayfirst=True)\n",
    "        return df\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao acessar a API do BCB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Buscando os dados da SELIC para o período relevante\n",
    "df_selic = get_selic_rate()\n",
    "\n",
    "print(\"\\n### Dados da Taxa SELIC (API do BCB) ###\")\n",
    "print(df_selic.info())\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "print(df_selic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35630a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizando a coluna de data do anúncio antes de mergear\n",
    "df_imoveis['data_anuncio'] = pd.to_datetime(df_imoveis['data_anuncio'], errors='coerce')\n",
    "\n",
    "# Criando chaves de junção (Ano-Mês)\n",
    "df_imoveis['ano_mes'] = df_imoveis['data_anuncio'].dt.to_period('M')\n",
    "df_selic['ano_mes'] = df_selic['data_selic'].dt.to_period('M')\n",
    "\n",
    "# Realizando o merge\n",
    "df_merged = pd.merge(df_imoveis, df_selic[['ano_mes', 'taxa_selic']], on='ano_mes', how='left')\n",
    "\n",
    "print(\"\\n### DataFrame Integrado ###\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores ausentes antes do tratamento:\")\n",
    "print(df_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para não alterar o original\n",
    "df_removido = df_merged.dropna()\n",
    "\n",
    "print(\"\\n--- Abordagem 1: Remoção ---\")\n",
    "print(f\"Tamanho do dataset original: {df_merged.shape}\")\n",
    "print(f\"Tamanho após remoção: {df_removido.shape}\")\n",
    "print(f\"Percentual de perda de dados: {((len(df_merged) - len(df_removido)) / len(df_merged)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para a imputação\n",
    "df_imputado = df_merged.copy()\n",
    "\n",
    "# Imputação da mediana para colunas numéricas (robusta a outliers)\n",
    "mediana_quartos = df_imputado['quartos'].median()\n",
    "mediana_area = df_imputado['area_m2'].median()\n",
    "mediana_preco = df_imputado['preco'].median()\n",
    "\n",
    "df_imputado['quartos'].fillna(mediana_quartos, inplace=True)\n",
    "df_imputado['area_m2'].fillna(mediana_area, inplace=True)\n",
    "df_imputado['preco'].fillna(mediana_preco, inplace=True)\n",
    "\n",
    "print(\"\\n--- Abordagem 2: Imputação ---\")\n",
    "print(\"Valores ausentes após imputação:\")\n",
    "print(df_imputado.isnull().sum())\n",
    "print(\"\\nEstatísticas descritivas (antes da imputação):\")\n",
    "print(df_merged.describe())\n",
    "print(\"\\nEstatísticas descritivas (após imputação):\")\n",
    "print(df_imputado.describe())\n",
    "\n",
    "#Decisão e Justificativa: A imputação pela mediana (para dados numéricos) foi escolhida. Ela preserva o tamanho do dataset e tem um impacto menor nas estatísticas descritivas do que a imputação pela média, especialmente na presença de outliers. A perda de 25% dos dados na abordagem de remoção seria muito prejudicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos o df_imputado como base\n",
    "df_tratado = df_imputado.copy()\n",
    "\n",
    "# Visualização antes do tratamento\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df_tratado['preco'])\n",
    "plt.title('Boxplot de Preços (Antes do Tratamento)')\n",
    "\n",
    "# Cálculo do IQR\n",
    "Q1 = df_tratado['preco'].quantile(0.25)\n",
    "Q3 = df_tratado['preco'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "\n",
    "print(f\"\\nLimite inferior para outliers de preço: R$ {limite_inferior:,.2f}\")\n",
    "print(f\"Limite superior para outliers de preço: R$ {limite_superior:,.2f}\")\n",
    "\n",
    "# Identificando o outlier\n",
    "outlier = df_tratado[df_tratado['preco'] > limite_superior]\n",
    "print(f\"\\nOutlier detectado:\\n{outlier}\")\n",
    "\n",
    "# Removendo o outlier\n",
    "df_tratado = df_tratado[df_tratado['preco'] <= limite_superior]\n",
    "\n",
    "# Visualização depois do tratamento\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df_tratado['preco'])\n",
    "plt.title('Boxplot de Preços (Após Tratamento)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#O imóvel com preço de R$ 15.000.000 foi identificado como um outlier significativo. Este valor poderia ser um erro de digitação ou um imóvel de altíssimo luxo que não representa o mercado geral. Optamos por removê-lo para evitar que ele influencie indevidamente as métricas estatísticas e o treinamento de futuros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Formato da coluna 'bairro' (antes):\")\n",
    "print(df_tratado['bairro'].unique())\n",
    "\n",
    "# Padronizando texto: minúsculas e sem espaços extras\n",
    "df_tratado['bairro'] = df_tratado['bairro'].str.lower().str.strip()\n",
    "\n",
    "print(\"\\nFormato da coluna 'bairro' (depois):\")\n",
    "print(df_tratado['bairro'].unique())\n",
    "\n",
    "# As colunas de data já foram padronizadas na etapa de integração\n",
    "print(\"\\nInformações do DataFrame final tratado:\")\n",
    "df_tratado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66421e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para a demonstração\n",
    "df_label_encoded = df_tratado.copy()\n",
    "\n",
    "# Instanciando e aplicando o encoder\n",
    "le = LabelEncoder()\n",
    "df_label_encoded['bairro_encoded'] = le.fit_transform(df_label_encoded['bairro'])\n",
    "\n",
    "print(\"--- LabelEncoder ---\")\n",
    "print(df_label_encoded[['bairro', 'bairro_encoded']].head())\n",
    "print(\"\\nMapeamento das classes:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2235cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia para a demonstração\n",
    "df_onehot_encoded = df_tratado.copy()\n",
    "\n",
    "# Instanciando o encoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Aplicando e criando um novo DataFrame com as colunas codificadas\n",
    "encoded_cols = ohe.fit_transform(df_onehot_encoded[['bairro']])\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=ohe.get_feature_names_out(['bairro']), index=df_onehot_encoded.index)\n",
    "\n",
    "# Concatenando com o DataFrame original\n",
    "df_onehot_encoded = pd.concat([df_onehot_encoded, encoded_df], axis=1)\n",
    "\n",
    "print(\"\\n--- OneHotEncoder ---\")\n",
    "print(df_onehot_encoded[['bairro', 'bairro_centro', 'bairro_zona leste', 'bairro_zona oeste', 'bairro_zona sul']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as colunas numéricas para escalar\n",
    "numeric_cols = df_tratado.select_dtypes(include=np.number).columns.drop('preco')\n",
    "df_scaled = df_onehot_encoded.copy()\n",
    "\n",
    "# Abordagem 1: MinMaxScaler (Normalização)\n",
    "# Transforma os dados para um intervalo [0, 1]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled['area_m2_minmax'] = scaler_minmax.fit_transform(df_scaled[['area_m2']])\n",
    "\n",
    "# Abordagem 2: StandardScaler (Padronização)\n",
    "# Transforma os dados para terem média 0 e desvio padrão 1\n",
    "scaler_standard = StandardScaler()\n",
    "df_scaled['area_m2_standard'] = scaler_standard.fit_transform(df_scaled[['area_m2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_scaled['area_m2'], kde=True, bins=10)\n",
    "plt.title('Distribuição Original (area_m2)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_scaled['area_m2_minmax'], kde=True, bins=10, color='green')\n",
    "plt.title('Após MinMaxScaler (Normalização)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df_scaled['area_m2_standard'], kde=True, bins=10, color='red')\n",
    "plt.title('Após StandardScaler (Padronização)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2db882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o DataFrame final da etapa de tratamento\n",
    "df_final = df_onehot_encoded.copy()\n",
    "\n",
    "# Atributo 1: Preço por metro quadrado (Razão)\n",
    "# Justificativa: Normaliza o preço pelo tamanho do imóvel, criando uma métrica de valor relativo. \n",
    "# Pode ser um forte indicador do padrão do bairro e do acabamento do imóvel.\n",
    "df_final['preco_por_m2'] = df_final['preco'] / df_final['area_m2']\n",
    "\n",
    "# Atributo 2: Idade do Anúncio (Data Feature)\n",
    "# Justificativa: A \"idade\" de um anúncio pode indicar sazonalidade no mercado ou a dificuldade\n",
    "# de venda. Usamos a data mais recente no dataset como referência.\n",
    "data_referencia = df_final['data_anuncio'].max()\n",
    "df_final['idade_anuncio_dias'] = (data_referencia - df_final['data_anuncio']).dt.days\n",
    "\n",
    "print(\"DataFrame com Novos Atributos:\")\n",
    "print(df_final[['area_m2', 'preco', 'preco_por_m2', 'data_anuncio', 'idade_anuncio_dias']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo Features (X) e Target (y)\n",
    "X = df_final.drop(['preco', 'id_imovel', 'data_anuncio', 'bairro', 'ano_mes', 'data_selic'], axis=1)\n",
    "y = df_final['preco']\n",
    "\n",
    "# Primeiro split: 70% treino, 30% temporário (val + teste)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Segundo split: 50% do temporário para validação, 50% para teste (15% do total cada)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"--- Divisão Holdout ---\")\n",
    "print(f\"Tamanho do conjunto de Treino: {X_train.shape[0]} amostras ({len(X_train)/len(df_final):.0%})\")\n",
    "print(f\"Tamanho do conjunto de Validação: {X_val.shape[0]} amostras ({len(X_val)/len(df_final):.0%})\")\n",
    "print(f\"Tamanho do conjunto de Teste: {X_test.shape[0]} amostras ({len(X_test)/len(df_final):.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bf167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold é aplicado sobre o conjunto de treino para validação robusta\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\n--- Validação Cruzada (K-Fold) ---\")\n",
    "print(\"Exemplo de índices para os 5 folds (aplicado sobre X_train):\")\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"  Índices de Treino: {train_index[:5]}...\") # Mostrando apenas os 5 primeiros\n",
    "    print(f\"  Índices de Validação: {val_index[:3]}...\") # Mostrando apenas os 3 primeiros\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4900878",
   "metadata": {},
   "source": [
    "<!-- Este projeto demonstrou um pipeline completo e profissional de preparação de dados. As principais decisões técnicas foram:\n",
    "\n",
    "Coleta de Dados: Enriquecemos dados locais de imóveis com a taxa SELIC via API do BCB, criando um dataset com contexto micro e macroeconômico.\n",
    "\n",
    "Limpeza de Dados: Optamos pela imputação pela mediana para tratar valores ausentes, preservando o tamanho do dataset. Outliers de preço foram removidos usando o critério de IQR para evitar distorções.\n",
    "\n",
    "Codificação Categórica: O OneHotEncoder foi escolhido para a variável bairro, pois evita a criação de uma relação ordinal artificial, o que é crucial para a maioria dos modelos.\n",
    "\n",
    "Escalonamento: O StandardScaler foi o método selecionado para padronizar as variáveis numéricas, por sua robustez e por ser um pré-requisito comum em muitos algoritmos de Machine Learning.\n",
    "\n",
    "Engenharia de Atributos: Foram criadas as features preco_por_m2 e idade_anuncio_dias, que agregam valor ao capturar relações de negócio importantes (valor relativo e temporalidade).\n",
    "\n",
    "Divisão dos Dados: A estratégia recomendada é a combinação de Holdout com K-Fold Cross-Validation. O Holdout isola um conjunto de teste final, enquanto o K-Fold é usado sobre os dados de treino para uma avaliação e otimização de modelo mais confiáveis.\n",
    "\n",
    "O DataFrame final está limpo, pré-processado e enriquecido, pronto para ser utilizado no treinamento de modelos de regressão para prever preços de imóveis com maior precisão e robustez. -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
